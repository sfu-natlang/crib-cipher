It is not trivial to get plain text data from Wikipedia.

The closest you can get is to use the WikiMedia API to get a XML document containing within a HTML rendering of the page in question.

For example, to get the page on "History", you would do this:
wget -O wiki.xml https://en.wikipedia.org/w/api.php?action=query\&prop=extracts\&titles=History\&format=xml
You now have wiki.xml, which contains the data from the article within XML.

To strip the outer XML, you can use the XMLStarlet toolkit.
xmlstarlet sel -T -t -c api wiki.xml > wiki.html

You now have wiki.html, which contains HTML of the article. Notably there are no anchors, only the text of the article is present.

From this point on, you can use html2text to quickly convent the data to markdown format.
Section headers can be now removed by grepping for lines not starting with a #.

#TODO: inner HTML -> Nuhn-type plaintext
